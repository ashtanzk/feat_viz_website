<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>What have we learnt?</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="index.html" class="title">Looking into the Blackbox</a>
				<nav>
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="index.html#one">Looking into the Blackbox</a></li>
						<li><a href="index.html#two">About us</a></li>
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
							<h1 class="major">What was going on?</h1>
							<!-- <span class="image fit"><img src="images/adversarial-banner.png" alt="" /></span> -->
							<h3>
								Fast Gradient Sign Method
							</h3>
							<p>The method that was used to produce the manipulated image is called the Fast Gradient Sign Method (FGSM).</p>
							<p>That is a form of adversarial attack on CNN models. This method works by generating a perturbation image whose pixel values are of the same direction as the gradient of the cost function with respect to the data. The noise is scaled by epsilon, which is usually constrained to be a small number via max norm.</p>
							<p>In normal CNN training, the gradients of the loss function are used to nudge the weights in the opposite direction, so as to minimise the loss value, getting the model closer to the actual class.</p>
							<p>In this case, we are making use of the opposite, but nudging the pixel values in the same direction as the loss function, maximising the loss, pushing the class further from the actual class.</p>
							<div class="img-showcase">
								<figure>
									<img src="images/adversarial-banner.png" style="width:100%; background-color: white; padding:0;">
								</figure>
							</div>
							<p>This method works by adding the generated perturbation image to the original image, resulting in the manipulated image.</p>
							<hr class="divider">
							<h3>
								Before we proceed
							</h3>
							<p>For the following sections, we will be using activation plots to show which filters are being activated by the model.</p>
							<div class="img-showcase">
								<figure>
									<img src="images/activation-plot.png" style="width:100%; background-color: white; padding:0;">
									<figcaption>Activation plot</figcaption>
								</figure>
							</div>
							<p>This plot is an activation plot. The y-axis represents the mean activation, while the x-axis represents the various filters present in the last convolutional layer of the model.</p>
							<p>From this plot, we are able to tell that the input image resulted in both filters 90 and 309 having a relatively high mean activation. That essentially means that the features present in those two filters are detected by the model from its convolutional layers.</p>
							<p>If we know that filter 90 is the filter with the hexagonal, and filter 309 is the filter with the pattern of round shapes, we can somewhat guess that the input image is quite close to a football.</p>
							<hr class="divider">
							<h3>
								Let us break it down
							</h3>
							<p>We shall now apply the FGSM to this input image of a broccoli to show the comparison between the how to model reacts to the original image compared to the manipulated image.</p>
							<div class="img-showcase">
								<figure>
									<img src="images/broccoli.jpg">
									<figcaption>Image of a broccoli</figcaption>
								</figure>
							</div>
							<p>We introduce a generated image by generating pixel values to be of the same direction as the gradient of the cost function with respect to the broccoli class, giving us this perturbation image.</p>
							<div class="img-showcase">
								<figure>
									<img src="images/broccoli-perturbation.jpg">
									<figcaption>Perturbation image</figcaption>
								</figure>
							</div>
							<p>We add this to the original image to generate the manipulated image of the broccoli, as shown below. You can see that the general image of the broccoli is still present, with some distortion added to the image.</p>
							<div class="img-showcase">
								<figure>
									<img src="images/broccoli-adversarial.jpg">
									<figcaption>Manipulated broccoli</figcaption>
								</figure>
							</div>
							<h3>
								What happens when you run both images through the model?
							</h3>
							<p>With this newly manipulated image of the broccoli, we can run this through the model to see the results. The original image and manipulated images will be run side by side to show the comparison.</p>
							<div class="img-showcase">
								<figure>
									<img src="images/broccoli.jpg">
									<figcaption>Image of a broccoli</figcaption>
								</figure>
								<figure>
									<img src="images/broccoli-adversarial.jpg">
									<figcaption>Manipulated broccoli</figcaption>
								</figure>
							</div>
							<div class="img-showcase">
								<figure>
									<img src="images/broccoli-plot.png" style="background-color: white;">
									<figcaption>Activation plot for original broccoli</figcaption>
								</figure>
								<figure>
									<img src="images/broccoli-adversarial-plot.png" style="background-color: white;">
									<figcaption>Activation plot for manipulated broccoli</figcaption>
								</figure>
							</div>
							<div class="img-showcase">
								<figure>
									<img src="images/broccoli-predictions.png" style="background-color: white;">
									<figcaption>Prediction scores for original broccoli</figcaption>
								</figure>
								<figure>
									<img src="images/broccoli-adversarial-predictions.png" style="background-color: white;">
									<figcaption>Prediction scores for manipulated broccoli</figcaption>
								</figure>
							</div>
							<p>In the original image, filter 475 has a very high mean activation compared to the rest of the filters. Let us assume that that particular filter is responsible for the broccoli classification.</p>
							<p>The manipulated image, on the other hand, sees the same filter have a much lower mean activation, resulting in it having a closer activation to several other layers. This results in the model changing the classification of the image to be a sombrero with a confidence of 29%</p>
							<p>To the human eye, the manipulated image still resembles a broccoli, with the addition of some form of distortion to the image. This distortion, which is attributed to the perturbation, is enough to lower the class activation of the broccoli feature map, effectively altering the classification of the image. </p>
							<hr class="divider">
							<h3>What is the impact, and how can we prevent it?</h3>
							<p>Adversarial attacks on CNN models can mean more than just lowering the accuracy of the model. When put into context, CNN models applied in fields such as autonomous vehicles are contingent on the models performing as they should be.</p>
							<p>What if an autonomous car is fed a manipulated image of a red light, resulting in the misclassification as a green light? That can lead to catastrophic consequences.</p>
							<p>As such, we should also know of some ways that we can mitigate this.</p>
							<ul>
								<li><strong>Adversarial training with perturbation or noise:</strong> Training on manipulated images to reduce misclassification</li>
								<li><strong>Gradient masking:</strong> Prevents attacker from access to the useful gradient</li>
								<li><strong>Ensemble adversarial learning:</strong> Training multiple classifiers and combining them to improve robustness</li>
							</ul>
							<p>This list is not exhaustive, and the methods depend on the type of model and adversarial attacks being applied.</p> 
							<hr class="divider">
							<h3>In summary</h3>
							<p>We have learnt that CNNs have three main parts: Input, Feature Learning, and Classification. Feature learning takes place in the CNN through the use of filters, ultimately resulting in the model learning many different feature maps.</p>
							<p>These feature maps are used by the model to determine if an image belongs to a certain class. We were able to make use of pixel value manipulation to visualise these feature maps, which gave us better insight as to how the model "recognises" patterns.</p>
							<p>Finally, we were able to piece together the knowledge of feature maps and pixel manipulation to introduce perturbations through an adversarial attack, which resulted in the model misclassifying the images.</p>
							<p>The use of the adversarial attack gave us valuable insight as to why explainability of CNN models is important, and how we can use that to our advantage.</p>
							<!-- <ul class="actions">
								<li><a href="index.html#one" class="button">Back</a></li>
							</ul> -->
							<hr class="divider">
							
							<ul class="actions">
								<li><a href="cnn-manipulate.html" class="button">Previous</a></li>
								<li><a href="index.html#two" class="button">End</a></li>
							</ul>
						</div>
					</section>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; Untitled. All rights reserved.</li><li>Design: HTML5 UP</li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>